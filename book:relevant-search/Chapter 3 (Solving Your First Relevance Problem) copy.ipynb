{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604a41dd",
   "metadata": {},
   "source": [
    "* code conversion: https://claude.ai/public/artifacts/f6064d7b-a650-403a-8a3a-3ebe51d55310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce209b07",
   "metadata": {},
   "source": [
    "# 3.2.2 Indexing TMDB Movies\n",
    "* load data from tmdb.json to open-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27d5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.10 environment at: /home/anon-labs/Documents/projects/search-nepali/.venv\u001b[0m\n",
      "Package                 Version\n",
      "----------------------- -----------\n",
      "aiohappyeyeballs        2.6.1\n",
      "aiohttp                 3.11.18\n",
      "aiosignal               1.3.2\n",
      "annotated-types         0.7.0\n",
      "anyio                   3.7.1\n",
      "asttokens               3.0.0\n",
      "attrs                   25.3.0\n",
      "blinker                 1.9.0\n",
      "certifi                 2025.4.26\n",
      "charset-normalizer      3.4.2\n",
      "click                   8.1.8\n",
      "comm                    0.2.2\n",
      "datasets                3.5.1\n",
      "debugpy                 1.8.14\n",
      "decorator               5.2.1\n",
      "dill                    0.3.8\n",
      "events                  0.5\n",
      "executing               2.2.0\n",
      "fastapi                 0.104.1\n",
      "filelock                3.18.0\n",
      "flask                   3.1.0\n",
      "flask-cors              5.0.1\n",
      "frozenlist              1.6.0\n",
      "fsspec                  2025.3.0\n",
      "h11                     0.16.0\n",
      "huggingface-hub         0.31.2\n",
      "idna                    3.10\n",
      "ipykernel               6.29.5\n",
      "ipython                 9.2.0\n",
      "ipython-pygments-lexers 1.1.1\n",
      "itsdangerous            2.2.0\n",
      "jedi                    0.19.2\n",
      "jinja2                  3.1.6\n",
      "jupyter-client          8.6.3\n",
      "jupyter-core            5.7.2\n",
      "markupsafe              3.0.2\n",
      "matplotlib-inline       0.1.7\n",
      "multidict               6.4.3\n",
      "multiprocess            0.70.16\n",
      "nest-asyncio            1.6.0\n",
      "numpy                   2.2.6\n",
      "opensearch-py           2.8.0\n",
      "packaging               25.0\n",
      "pandas                  2.2.3\n",
      "parso                   0.8.4\n",
      "pexpect                 4.9.0\n",
      "platformdirs            4.3.8\n",
      "prompt-toolkit          3.0.51\n",
      "propcache               0.3.1\n",
      "psutil                  7.0.0\n",
      "ptyprocess              0.7.0\n",
      "pure-eval               0.2.3\n",
      "pyarrow                 20.0.0\n",
      "pydantic                2.11.4\n",
      "pydantic-core           2.33.2\n",
      "pydantic-settings       2.9.1\n",
      "pygments                2.19.1\n",
      "python-dateutil         2.9.0.post0\n",
      "python-dotenv           1.1.0\n",
      "pytz                    2025.2\n",
      "pyyaml                  6.0.2\n",
      "pyzmq                   26.4.0\n",
      "requests                2.32.3\n",
      "six                     1.17.0\n",
      "sniffio                 1.3.1\n",
      "stack-data              0.6.3\n",
      "starlette               0.27.0\n",
      "tornado                 6.5\n",
      "tqdm                    4.67.1\n",
      "traitlets               5.14.3\n",
      "typing-extensions       4.13.2\n",
      "typing-inspection       0.4.0\n",
      "tzdata                  2025.2\n",
      "urllib3                 2.4.0\n",
      "uvicorn                 0.24.0\n",
      "wcwidth                 0.2.13\n",
      "werkzeug                3.1.3\n",
      "xxhash                  3.5.0\n",
      "yarl                    1.20.0\n",
      "zstandard               0.23.0\n"
     ]
    }
   ],
   "source": [
    "!uv pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e71456",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index\n",
      "Created index: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'tmdb'}\n",
      "Building bulk request...\n",
      "Indexing documents...\n",
      "Successfully indexed 3051 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract():\n",
    "    f = open('tmdb.json')\n",
    "    if f:\n",
    "        return json.loads(f.read())        \n",
    "    return {}\n",
    "\n",
    "def reindex(analysisSettings={}, mappingSettings={}, movieDict={}):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"index\": {\n",
    "                \"analysis\": analysisSettings,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if mappingSettings:\n",
    "        settings['mappings'] = mappingSettings\n",
    "    \n",
    "    # Delete index if it exists\n",
    "    try:\n",
    "        client.indices.delete(index='tmdb')\n",
    "        print(\"Deleted existing index\")\n",
    "    except:\n",
    "        pass  # Index might not exist yet\n",
    "    \n",
    "    # Create new index with settings\n",
    "    response = client.indices.create('tmdb', body=settings)\n",
    "    print(\"Created index:\", response)\n",
    "    \n",
    "    print(\"Building bulk request...\")\n",
    "    bulk_data = []\n",
    "    for id, movie in movieDict.items():  # Using items() instead of iteritems() for Python 3\n",
    "        # Add the indexing command\n",
    "        bulk_data.append({\n",
    "            \"index\": {\n",
    "                \"_index\": \"tmdb\",\n",
    "                \"_id\": movie[\"id\"]\n",
    "            }\n",
    "        })\n",
    "        # Add the document to index\n",
    "        bulk_data.append(movie)\n",
    "    \n",
    "    # Only perform bulk operation if there's data to index\n",
    "    if bulk_data:\n",
    "        print(\"Indexing documents...\")\n",
    "        response = client.bulk(body=bulk_data)\n",
    "        \n",
    "        # Check if there were any errors\n",
    "        if response.get('errors', False):\n",
    "            print(\"Errors during bulk indexing:\", response)\n",
    "        else:\n",
    "            print(f\"Successfully indexed {len(bulk_data)//2} documents\")\n",
    "    else:\n",
    "        print(\"No documents to index\")\n",
    "\n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "    movieDict = extract()\n",
    "    reindex(movieDict=movieDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a09fc",
   "metadata": {},
   "source": [
    "# 3.2.3 Basic Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b668d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num\tRelevance Score\t\tMovie Title\t\tOverview\n",
      "1\t38.895134\t\tAliens\n",
      "2\t33.504898\t\tThe Basketball Diaries\n",
      "3\t32.418274\t\tCowboys & Aliens\n",
      "4\t27.790558\t\tMonsters vs Aliens\n",
      "5\t24.319012\t\tAliens vs Predator: Requiem\n",
      "6\t24.319012\t\tAliens in the Attic\n",
      "7\t20.555042\t\tDances with Wolves\n",
      "8\t20.555042\t\tFriends with Benefits\n",
      "9\t20.555042\t\tFire with Fire\n",
      "10\t20.555042\t\tFriends with Kids\n",
      "11\t17.987345\t\tInterview with the Vampire\n",
      "12\t17.987345\t\tFrom Russia With Love\n",
      "13\t17.987345\t\tGone with the Wind\n",
      "14\t17.987345\t\tJust Go With It\n",
      "15\t17.987345\t\tMy Week with Marilyn\n",
      "16\t17.987345\t\tFrom Paris with Love\n",
      "17\t17.987345\t\tTrouble with the Curve\n",
      "18\t17.987345\t\tSleeping with the Enemy\n",
      "19\t17.987345\t\tHobo with a Shotgun\n",
      "20\t17.987345\t\tTo Rome with Love\n",
      "21\t15.989916\t\tDie Hard: With a Vengeance\n",
      "22\t15.989916\t\tGirl with a Pearl Earring\n",
      "23\t15.989916\t\tFun with Dick and Jane\n",
      "24\t14.391762\t\tThe Girl with the Dragon Tattoo\n",
      "25\t14.391762\t\tThe Life Aquatic With Steve Zissou\n",
      "26\t14.391762\t\tTwin Peaks: Fire Walk with Me\n",
      "27\t14.391762\t\tYou Don't Mess With the Zohan\n",
      "28\t14.391762\t\tThe Man with the Golden Gun\n",
      "29\t14.391762\t\tCloudy with a Chance of Meatballs\n",
      "30\t14.391762\t\tThe Girl with the Dragon Tattoo\n",
      "31\t14.391762\t\tThe Man with the Iron Fists\n",
      "32\t14.391762\t\tThe Girl Who Played with Fire\n",
      "33\t13.084042\t\tCloudy with a Chance of Meatballs 2\n",
      "34\t13.084042\t\tThe Pirates! In an Adventure with Scientists!\n",
      "35\t4.2295613\t\tMeet Dave\n",
      "36\t3.7533708\t\tSpeed Racer\n",
      "37\t3.6251724\t\tSpace Jam\n",
      "38\t3.5147643\t\tGrown Ups\n",
      "39\t3.4117591\t\tSemi-Pro\n",
      "40\t3.359213\t\tThe Flintstones\n",
      "41\t3.281564\t\tCoach Carter\n",
      "42\t3.0502703\t\tWhite Men Can't Jump\n",
      "43\t3.0314803\t\tAlien: Resurrection\n",
      "44\t2.969013\t\tTeen Wolf\n",
      "45\t2.9670591\t\tDistrict 9\n",
      "46\t2.9508166\t\tBedazzled\n",
      "47\t2.9356256\t\tThe Watch\n",
      "48\t2.7980134\t\tGalaxy Quest\n",
      "49\t2.6122577\t\tBatteries Not Included\n",
      "50\t2.5007267\t\tThey Live\n",
      "51\t2.412089\t\tPitch Black\n",
      "52\t2.3313658\t\tHigh School Musical\n",
      "53\t2.3060656\t\tAVP: Alien vs. Predator\n",
      "54\t2.240414\t\tDude, Where’s My Car?\n",
      "55\t2.2167134\t\tMen in Black 3\n",
      "56\t2.1783967\t\tBattlefield Earth\n",
      "57\t2.1783967\t\tCocoon\n",
      "58\t0.7438285\t\tDark City\n",
      "59\t0.7284197\t\tSex Drive\n",
      "60\t0.7247722\t\tThe Switch\n",
      "61\t0.7247722\t\tCarrie\n",
      "62\t0.7191612\t\tNim's Island\n",
      "63\t0.7191612\t\tBride of Chucky\n",
      "64\t0.7191612\t\tYoung Adult\n",
      "65\t0.7146197\t\tWhite Noise\n",
      "66\t0.7146197\t\tThe Guard\n",
      "67\t0.7146197\t\tOdd Thomas\n",
      "68\t0.713841\t\tFrida\n",
      "69\t0.706668\t\tGuess Who\n",
      "70\t0.7057065\t\tSilver Linings Playbook\n",
      "71\t0.7013328\t\tCinema Paradiso\n",
      "72\t0.6950928\t\tThe Man in the Iron Mask\n",
      "73\t0.692746\t\tStrangers on a Train\n",
      "74\t0.692746\t\t[REC]²\n",
      "75\t0.692746\t\tWild Card\n",
      "76\t0.692746\t\tSafe Haven\n",
      "77\t0.68436694\t\tStep Up Revolution\n",
      "78\t0.6838906\t\tGhosts of Girlfriends Past\n",
      "79\t0.6802529\t\tMy Big Fat Greek Wedding\n",
      "80\t0.6802529\t\t27 Dresses\n",
      "81\t0.6784239\t\tLethal Weapon\n",
      "82\t0.6784239\t\tArmored\n",
      "83\t0.6721716\t\tVicky Cristina Barcelona\n",
      "84\t0.66774845\t\tGarden State\n",
      "85\t0.66774845\t\tMary Poppins\n",
      "86\t0.66774845\t\tThe Ant Bully\n",
      "87\t0.66774845\t\tDays of Thunder\n",
      "88\t0.66774845\t\tRush Hour 3\n",
      "89\t0.66428\t\t8½\n",
      "90\t0.66428\t\tFantastic Mr. Fox\n",
      "91\t0.66428\t\tThe Spiderwick Chronicles\n",
      "92\t0.6636071\t\tJustice League: Throne of Atlantis\n",
      "93\t0.66040325\t\tJust Friends\n",
      "94\t0.66040325\t\tStuart Little\n",
      "95\t0.65740377\t\tJersey Girl\n",
      "96\t0.65740377\t\tMatilda\n",
      "97\t0.65657157\t\tSubmarine\n",
      "98\t0.65278405\t\tOutlander\n",
      "99\t0.65278405\t\tStoker\n",
      "100\t0.65278405\t\tMesrine: Killer Instinct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def search(query):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Perform search using the OpenSearch client\n",
    "    response = client.search(\n",
    "        body=query,\n",
    "        index='tmdb'\n",
    "    )\n",
    "    \n",
    "    # Extract search hits\n",
    "    search_hits = response['hits']\n",
    "    \n",
    "    # Print results in a formatted table\n",
    "    print(\"Num\\tRelevance Score\\t\\tMovie Title\\t\\tOverview\")\n",
    "    for idx, hit in enumerate(search_hits['hits']):\n",
    "        print(f\"{idx + 1}\\t{hit['_score']}\\t\\t{hit['_source']['title']}\")\n",
    "    \n",
    "    # Return the raw response for further processing if needed\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    users_search = 'basketball with cartoon aliens'\n",
    "    query = {\n",
    "        'query': {\n",
    "            'multi_match': { \n",
    "                'query': users_search,  # The user's search string\n",
    "                'fields': ['title^10', 'overview'],  # Fields to search, with title boosted\n",
    "            },\n",
    "        },\n",
    "        'size': 100  # Number of results to return\n",
    "    }\n",
    "    search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fc254",
   "metadata": {},
   "source": [
    "# 2.3.1 Query Validation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fa3c6f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_shards': {'total': 1, 'successful': 1, 'failed': 0}, 'valid': True, 'explanations': [{'index': 'tmdb', 'valid': True, 'explanation': '((overview:basketball overview:with overview:cartoon overview:aliens) | (title:basketball title:with title:cartoon title:aliens)^10.0)'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def validate_query(query, explain=True):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Validate the query using the OpenSearch client's indices.validate_query method\n",
    "    response = client.indices.validate_query(\n",
    "        body=query,\n",
    "        index='tmdb',\n",
    "        explain=explain\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    users_search = 'basketball with cartoon aliens'\n",
    "    query = {\n",
    "        'query': {\n",
    "            'multi_match': { \n",
    "                'query': users_search,  # User's query\n",
    "                'fields': ['title^10', 'overview']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    validate_query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f92c6b",
   "metadata": {},
   "source": [
    "# 2.3.3 Debugging Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9303643c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING TEXT 'Fire with Fire' IN TITLE FIELD:\n",
      "tokens:\n",
      "- end_offset: 4\n",
      "  position: 0\n",
      "  start_offset: 0\n",
      "  token: fire\n",
      "  type: <ALPHANUM>\n",
      "- end_offset: 9\n",
      "  position: 1\n",
      "  start_offset: 5\n",
      "  token: with\n",
      "  type: <ALPHANUM>\n",
      "- end_offset: 14\n",
      "  position: 2\n",
      "  start_offset: 10\n",
      "  token: fire\n",
      "  type: <ALPHANUM>\n",
      "\n",
      "\n",
      "Tokens generated:\n",
      "  fire (position: 0, start: 0, end: 4)\n",
      "  with (position: 1, start: 5, end: 9)\n",
      "  fire (position: 2, start: 10, end: 14)\n",
      "\n",
      "GETTING MAPPING FOR TITLE FIELD:\n",
      "tmdb:\n",
      "  mappings:\n",
      "    title:\n",
      "      full_name: title\n",
      "      mapping:\n",
      "        title:\n",
      "          fields:\n",
      "            keyword:\n",
      "              ignore_above: 256\n",
      "              type: keyword\n",
      "          type: text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def analyze_text(index, field, text):\n",
    "    \"\"\"\n",
    "    Analyze how the specified text would be tokenized for a given field in the index.\n",
    "    This helps understand:\n",
    "    (1) What tokens are placed in the search engine\n",
    "    (2) What the search engine attempts to match exactly\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Build the analyze request body\n",
    "    body = {\n",
    "        \"field\": field,\n",
    "        \"text\": text\n",
    "    }\n",
    "    \n",
    "    # Perform the analysis\n",
    "    response = client.indices.analyze(\n",
    "        index=index,\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    # Format and print the response as YAML for better readability\n",
    "    yaml_response = yaml.dump(response, default_flow_style=False)\n",
    "    print(yaml_response)\n",
    "    \n",
    "    # Also print a more concise summary of just the tokens\n",
    "    print(\"\\nTokens generated:\")\n",
    "    for token in response['tokens']:\n",
    "        print(f\"  {token['token']} (position: {token['position']}, start: {token['start_offset']}, end: {token['end_offset']})\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_field_mapping(index, field):\n",
    "    \"\"\"\n",
    "    Get the mapping details for a specific field in an index\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Get the field mapping\n",
    "    response = client.indices.get_field_mapping(\n",
    "        index=index,\n",
    "        fields=field\n",
    "    )\n",
    "    \n",
    "    # Format and print the response as YAML for better readability\n",
    "    yaml_response = yaml.dump(response, default_flow_style=False)\n",
    "    print(yaml_response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    \n",
    "    # 1. Analyze how \"Fire with Fire\" would be tokenized in the title field\n",
    "    print(\"ANALYZING TEXT 'Fire with Fire' IN TITLE FIELD:\")\n",
    "    analyze_text(\"tmdb\", \"title\", \"Fire with Fire\")\n",
    "    \n",
    "    # 2. Optionally, get the mapping details for the title field\n",
    "    print(\"\\nGETTING MAPPING FOR TITLE FIELD:\")\n",
    "    get_field_mapping(\"tmdb\", \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c230b2",
   "metadata": {},
   "source": [
    "# 2.3.5 -- Solving The Matching Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cead66ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index\n",
      "Created index with mappings: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'tmdb'}\n",
      "Building bulk request...\n",
      "Indexing documents...\n",
      "Successfully indexed 3051 documents\n",
      "\n",
      "Analyzing 'Fire with Fire' with the new mappings:\n",
      "tokens:\n",
      "- end_offset: 4\n",
      "  position: 0\n",
      "  start_offset: 0\n",
      "  token: fire\n",
      "  type: <ALPHANUM>\n",
      "- end_offset: 14\n",
      "  position: 2\n",
      "  start_offset: 10\n",
      "  token: fire\n",
      "  type: <ALPHANUM>\n",
      "\n",
      "\n",
      "Tokens generated:\n",
      "  fire (position: 0, start: 0, end: 4)\n",
      "  fire (position: 2, start: 10, end: 14)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract():\n",
    "    \"\"\"Extract movie data from tmdb.json file\"\"\"\n",
    "    f = open('tmdb.json')\n",
    "    if f:\n",
    "        return json.loads(f.read())        \n",
    "    return {}\n",
    "\n",
    "def reindex(analysisSettings={}, mappingSettings={}, movieDict={}):\n",
    "    \"\"\"\n",
    "    Reindex the movie data with specified mappings and settings\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"index\": {\n",
    "                \"analysis\": analysisSettings,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add mappings if provided\n",
    "    if mappingSettings:\n",
    "        # Convert format to match OpenSearch (remove document type layer for OpenSearch ≥ 2.0)\n",
    "        # In OpenSearch, the mapping structure is flattened compared to older Elasticsearch\n",
    "        if 'movie' in mappingSettings:\n",
    "            settings['mappings'] = mappingSettings['movie']\n",
    "        else:\n",
    "            settings['mappings'] = mappingSettings\n",
    "    \n",
    "    # Delete index if it exists\n",
    "    try:\n",
    "        client.indices.delete(index='tmdb')\n",
    "        print(\"Deleted existing index\")\n",
    "    except:\n",
    "        pass  # Index might not exist yet\n",
    "    \n",
    "    # Create new index with settings\n",
    "    response = client.indices.create('tmdb', body=settings)\n",
    "    print(\"Created index with mappings:\", response)\n",
    "    \n",
    "    print(\"Building bulk request...\")\n",
    "    bulk_data = []\n",
    "    for id, movie in movieDict.items():  # Using items() instead of iteritems() for Python 3\n",
    "        # Add the indexing command\n",
    "        bulk_data.append({\n",
    "            \"index\": {\n",
    "                \"_index\": \"tmdb\",\n",
    "                \"_id\": movie[\"id\"]\n",
    "            }\n",
    "        })\n",
    "        # Add the document to index\n",
    "        bulk_data.append(movie)\n",
    "    \n",
    "    # Only perform bulk operation if there's data to index\n",
    "    if bulk_data:\n",
    "        print(\"Indexing documents...\")\n",
    "        response = client.bulk(body=bulk_data)\n",
    "        \n",
    "        # Check if there were any errors\n",
    "        if response.get('errors', False):\n",
    "            print(\"Errors during bulk indexing:\", response)\n",
    "        else:\n",
    "            print(f\"Successfully indexed {len(bulk_data)//2} documents\")\n",
    "    else:\n",
    "        print(\"No documents to index\")\n",
    "\n",
    "def analyze_text(index, field, text):\n",
    "    \"\"\"\n",
    "    Analyze how the specified text would be tokenized for a given field in the index.\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    # Build the analyze request body\n",
    "    body = {\n",
    "        \"field\": field,\n",
    "        \"text\": text\n",
    "    }\n",
    "    \n",
    "    # Perform the analysis\n",
    "    response = client.indices.analyze(\n",
    "        index=index,\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    # Format and print the response as YAML\n",
    "    yaml_response = yaml.dump(response, default_flow_style=False)\n",
    "    print(yaml_response)\n",
    "    \n",
    "    # Also print a concise summary of just the tokens\n",
    "    print(\"\\nTokens generated:\")\n",
    "    for token in response['tokens']:\n",
    "        print(f\"  {token['token']} (position: {token['position']}, start: {token['start_offset']}, end: {token['end_offset']})\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the mapping settings with proper text field types and analyzers\n",
    "    # Note: In OpenSearch, 'text' is used instead of 'string' type\n",
    "    mapping_settings = {\n",
    "        'movie': {\n",
    "            'properties': {\n",
    "                'title': {\n",
    "                    'type': 'text',  # 'string' is deprecated, use 'text' instead\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'overview': {\n",
    "                    'type': 'text',  # 'string' is deprecated, use 'text' instead\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract movie data and reindex with proper mappings\n",
    "    movie_dict = extract()\n",
    "    reindex(mappingSettings=mapping_settings, movieDict=movie_dict)\n",
    "    \n",
    "    # Analyze how \"Fire with Fire\" would be tokenized with the new mapping\n",
    "    print(\"\\nAnalyzing 'Fire with Fire' with the new mappings:\")\n",
    "    analyze_text(\"tmdb\", \"title\", \"Fire with Fire\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ea02ce7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"fire\"\n",
      "  start_offset: 0\n",
      "  end_offset: 4\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 0\n",
      "- token: \"fire\"\n",
      "  start_offset: 10\n",
      "  end_offset: 14\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://localhost:9200/tmdb/_analyze?field=title&format=yaml', \n",
    "                    data=\"Fire with Fire\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4f16d",
   "metadata": {},
   "source": [
    "## Repeat the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af7e882a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index\n",
      "Created index with mappings: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'tmdb'}\n",
      "Building bulk request...\n",
      "Indexing documents...\n",
      "Successfully indexed 3051 documents\n",
      "\n",
      "Searching for: 'basketball with cartoon aliens'\n",
      "\n",
      "Search Results:\n",
      "Num\tRelevance Score\t\tMovie Title\t\tOverview\n",
      "1\t34.2826\t\tAlien\t\tDuring its return to the earth, commercial spacesh...\n",
      "2\t34.2826\t\tAliens\t\tWhen Ripley's lifepod is found by a salvage crew o...\n",
      "3\t33.7776\t\tThe Basketball Diaries\t\tFilm adaptation of street tough Jim Carroll's epis...\n",
      "4\t27.6502\t\tCowboys & Aliens\t\tA stranger stumbles into the desert town of Absolu...\n",
      "5\t19.9364\t\tAliens vs Predator: Requiem\t\tA sequel to 2004's Alien vs. Predator, the iconic ...\n",
      "6\t19.9364\t\tAVP: Alien vs. Predator\t\tWhen scientists discover something in the Arctic t...\n",
      "7\t5.8666\t\tSpace Jam\t\tMichael Jordan agrees to help the Looney Tunes pla...\n",
      "8\t3.2702\t\tThe Flintstones\t\tModern Stone Age family the Flintstones hit the bi...\n",
      "9\t3.0565\t\tWhite Men Can't Jump\t\tBilly Hoyle (Woody Harrelson) and Sidney Deane (We...\n",
      "10\t2.6920\t\tMeet Dave\t\tA crew of miniature aliens operate a spaceship tha...\n",
      "11\t2.4552\t\tBedazzled\t\tElliot Richardson, suicidal techno geek, is given ...\n",
      "12\t2.4525\t\tThe Thing\t\tScientists in the Antarctic are confronted by a sh...\n",
      "13\t2.4054\t\tHigh School Musical\t\tTroy (Zac Efron), the popular captain of the baske...\n",
      "14\t2.3765\t\tThe Darkest Hour\t\tThe story tells of a group of young people who fig...\n",
      "15\t2.3765\t\tInvasion of the Body Snatchers\t\tA small-town doctor learns that the population of ...\n",
      "16\t2.2674\t\tTeen Wolf\t\tWhat's a high school kid got to do to be popular? ...\n",
      "17\t2.2401\t\tIndependence Day\t\tOn July 2, a giant alien mothership enters orbit a...\n",
      "18\t2.2379\t\tEscape from Planet Earth\t\tAstronaut Scorch Supernova finds himself caught in...\n",
      "19\t2.2058\t\tAvatar\t\tIn the 22nd century, a paraplegic Marine is dispat...\n",
      "20\t2.1981\t\tThe X Files\t\tBlackwood, Texas: After falling through a hole in ...\n",
      "21\t2.1146\t\tThe Last Starfighter\t\tA video-gaming boy, seemingly doomed to stay at hi...\n",
      "22\t2.0579\t\tThe Day the Earth Stood Still\t\tAn alien and a robot land on earth after World War...\n",
      "23\t2.0059\t\tStar Trek IV: The Voyage Home\t\tFugitives of the Federation for their daring rescu...\n",
      "24\t2.0059\t\tGhosts of Mars\t\tMelanie Ballard (Natasha Henstridge) is a hard nos...\n",
      "25\t2.0042\t\tOutlander\t\tDuring the reign of the Vikings, a man from anothe...\n",
      "26\t1.9783\t\tNapoleon Dynamite\t\tA listless and alienated teenager decides to help ...\n",
      "27\t1.9548\t\tAttack the Block\t\tA teen gang in South London defend their block fro...\n",
      "28\t1.9286\t\tGalaxy Quest\t\tDecades after the success of the sci-fi series \"Ga...\n",
      "29\t1.8585\t\tContact\t\tContact is a science fiction film about an encount...\n",
      "30\t1.8585\t\tStar Trek: Insurrection\t\tWhen an alien race and factions within Starfleet a...\n",
      "31\t1.8585\t\tBatteries Not Included\t\tIn a soon to be demolished block of apartments, th...\n",
      "32\t1.7327\t\tMars Attacks!\t\t'We come in peace' is not what those green men fro...\n",
      "33\t1.7133\t\tPredators\t\tA mercenary named Royce reluctantly leads a motley...\n",
      "34\t1.6578\t\tThe Day the Earth Stood Still\t\tA representative of an alien race that went throug...\n",
      "35\t1.6578\t\tPitch Black\t\tAfter crash-landing on a seemingly lifeless planet...\n",
      "36\t1.6227\t\tBattlefield Earth\t\tIn the year 3000, man is no match for the Psychlo'...\n",
      "37\t1.6227\t\tCocoon\t\tA group of aliens return to earth to take back som...\n",
      "38\t1.6227\t\tLilo & Stitch\t\tA lonely Hawaiian girl named Lilo is being raised ...\n",
      "39\t1.6227\t\tSpider-Man 3\t\tThe seemingly invincible Spider-Man goes up agains...\n",
      "40\t1.5569\t\tShort Circuit\t\tAfter a lightning bolt zaps a robot named Number 5...\n",
      "41\t1.5259\t\tPredator 2\t\tTen years after a band of mercenaries first battle...\n",
      "42\t1.5259\t\tThey Live\t\tNada, a down-on-his-luck construction worker, disc...\n",
      "43\t1.5259\t\tThe Faculty\t\tWhen some very creepy things start happening aroun...\n",
      "44\t1.4676\t\tPredator\t\tDutch and his group of commandos are hired by the ...\n",
      "45\t1.4135\t\tMen in Black\t\tMen in Black follows the exploits of agents Kay an...\n",
      "46\t1.4135\t\tMonsters\t\tSix years ago NASA discovered the possibility of a...\n",
      "47\t1.4135\t\tHome\t\tWhen Earth is taken over by the overly-confident B...\n",
      "48\t1.2728\t\tMen in Black 3\t\tAgents J (Will Smith) and K (Tommy Lee Jones) are ...\n",
      "49\t1.2320\t\tMen in Black II\t\tKay and Jay reunite to provide our best, last and ...\n",
      "50\t1.1237\t\tStalker\t\tNear a gray and unnamed city is the Zone, an alien...\n",
      "\n",
      "Total results: 50\n",
      "\n",
      "Query explanation:\n",
      "\n",
      "How the search term 'basketball with cartoon aliens' is analyzed:\n",
      "Tokens that will be matched against the index:\n",
      "  - basketbal\n",
      "  - cartoon\n",
      "  - alien\n",
      "\n",
      "Note: The English analyzer removes stopwords like 'with' and stems words to their root form.\n",
      "The search will match documents where these tokens appear in either title (weighted higher) or overview.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract():\n",
    "    \"\"\"Extract movie data from tmdb.json file\"\"\"\n",
    "    f = open('tmdb.json')\n",
    "    if f:\n",
    "        return json.loads(f.read())        \n",
    "    return {}\n",
    "\n",
    "def setup_client():\n",
    "    \"\"\"Set up and return an OpenSearch client\"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    return client\n",
    "\n",
    "def reindex(analysisSettings={}, mappingSettings={}, movieDict={}):\n",
    "    \"\"\"\n",
    "    Reindex the movie data with specified mappings and settings\n",
    "    \"\"\"\n",
    "    client = setup_client()\n",
    "    \n",
    "    # Index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"index\": {\n",
    "                \"analysis\": analysisSettings,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add mappings if provided\n",
    "    if mappingSettings:\n",
    "        # Convert format to match OpenSearch (remove document type layer for OpenSearch ≥ 2.0)\n",
    "        if 'movie' in mappingSettings:\n",
    "            settings['mappings'] = mappingSettings['movie']\n",
    "        else:\n",
    "            settings['mappings'] = mappingSettings\n",
    "    \n",
    "    # Delete index if it exists\n",
    "    try:\n",
    "        client.indices.delete(index='tmdb')\n",
    "        print(\"Deleted existing index\")\n",
    "    except:\n",
    "        pass  # Index might not exist yet\n",
    "    \n",
    "    # Create new index with settings\n",
    "    response = client.indices.create('tmdb', body=settings)\n",
    "    print(\"Created index with mappings:\", response)\n",
    "    \n",
    "    print(\"Building bulk request...\")\n",
    "    bulk_data = []\n",
    "    for id, movie in movieDict.items():\n",
    "        # Add the indexing command\n",
    "        bulk_data.append({\n",
    "            \"index\": {\n",
    "                \"_index\": \"tmdb\",\n",
    "                \"_id\": movie[\"id\"]\n",
    "            }\n",
    "        })\n",
    "        # Add the document to index\n",
    "        bulk_data.append(movie)\n",
    "    \n",
    "    # Only perform bulk operation if there's data to index\n",
    "    if bulk_data:\n",
    "        print(\"Indexing documents...\")\n",
    "        response = client.bulk(body=bulk_data)\n",
    "        \n",
    "        # Check if there were any errors\n",
    "        if response.get('errors', False):\n",
    "            print(\"Errors during bulk indexing:\", response)\n",
    "        else:\n",
    "            print(f\"Successfully indexed {len(bulk_data)//2} documents\")\n",
    "    else:\n",
    "        print(\"No documents to index\")\n",
    "\n",
    "def search(query):\n",
    "    \"\"\"\n",
    "    Search the tmdb index with the given query\n",
    "    \"\"\"\n",
    "    client = setup_client()\n",
    "    \n",
    "    # Perform search using the OpenSearch client\n",
    "    response = client.search(\n",
    "        body=query,\n",
    "        index='tmdb'\n",
    "    )\n",
    "    \n",
    "    # Extract search hits\n",
    "    search_hits = response['hits']\n",
    "    \n",
    "    # Print results in a formatted table\n",
    "    print(\"\\nSearch Results:\")\n",
    "    print(\"Num\\tRelevance Score\\t\\tMovie Title\\t\\tOverview\")\n",
    "    for idx, hit in enumerate(search_hits['hits']):\n",
    "        # Format the overview for better display (limit length)\n",
    "        overview = hit['_source'].get('overview', '')\n",
    "        if len(overview) > 50:\n",
    "            overview = overview[:50] + \"...\"\n",
    "            \n",
    "        print(f\"{idx + 1}\\t{hit['_score']:.4f}\\t\\t{hit['_source']['title']}\\t\\t{overview}\")\n",
    "    \n",
    "    print(f\"\\nTotal results: {search_hits['total']['value']}\")\n",
    "    \n",
    "    # Return the raw response for further processing if needed\n",
    "    return response\n",
    "\n",
    "def run_search_demo():\n",
    "    \"\"\"Run a complete demo of indexing with proper mappings and searching\"\"\"\n",
    "    # 1. Define the mapping settings with proper text field types and analyzers\n",
    "    mapping_settings = {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': 'text',  # 'string' is deprecated, use 'text' instead\n",
    "                'analyzer': 'english'\n",
    "            },\n",
    "            'overview': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'english'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 2. Extract movie data and reindex with proper mappings\n",
    "    movie_dict = extract()\n",
    "    if movie_dict:\n",
    "        reindex(mappingSettings=mapping_settings, movieDict=movie_dict)\n",
    "        \n",
    "        # 3. Perform the search\n",
    "        users_search = 'basketball with cartoon aliens'\n",
    "        query = {\n",
    "            'query': {\n",
    "                'multi_match': { \n",
    "                    'query': users_search,\n",
    "                    'fields': ['title^10', 'overview'],  # title has 10x the weight\n",
    "                },\n",
    "            },\n",
    "            'size': 100  # Number of results to return (as integer)\n",
    "        }\n",
    "        \n",
    "        # 4. Execute search and get results\n",
    "        print(f\"\\nSearching for: '{users_search}'\")\n",
    "        search(query)\n",
    "        \n",
    "        # 5. Show query explanation (optional)\n",
    "        print(\"\\nQuery explanation:\")\n",
    "        explain_query(query)\n",
    "    else:\n",
    "        print(\"No movie data found. Please ensure the tmdb.json file exists.\")\n",
    "\n",
    "def explain_query(query):\n",
    "    \"\"\"\n",
    "    Explain how the query works with the current mappings\n",
    "    \"\"\"\n",
    "    client = setup_client()\n",
    "    users_search = query['query']['multi_match']['query']\n",
    "    \n",
    "    # Analyze the query terms with the title field analyzer\n",
    "    analyze_body = {\n",
    "        \"field\": \"title\",\n",
    "        \"text\": users_search\n",
    "    }\n",
    "    \n",
    "    # Perform the analysis\n",
    "    response = client.indices.analyze(\n",
    "        index='tmdb',\n",
    "        body=analyze_body\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHow the search term '{users_search}' is analyzed:\")\n",
    "    print(\"Tokens that will be matched against the index:\")\n",
    "    for token in response['tokens']:\n",
    "        print(f\"  - {token['token']}\")\n",
    "    \n",
    "    print(\"\\nNote: The English analyzer removes stopwords like 'with' and stems words to their root form.\")\n",
    "    print(\"The search will match documents where these tokens appear in either title (weighted higher) or overview.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_search_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba044b32",
   "metadata": {},
   "source": [
    "# 2.4.1\tDecomposing Relevance Score With Lucene’s Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f039791",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index\n",
      "Created index with mappings: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'tmdb'}\n",
      "Building bulk request...\n",
      "Indexing documents...\n",
      "Successfully indexed 3051 documents\n",
      "\n",
      "================================================================================\n",
      "STEP 1: INITIAL SEARCH\n",
      "================================================================================\n",
      "Searching for: 'basketball with cartoon aliens'\n",
      "\n",
      "Search Results:\n",
      "Num\tRelevance Score\t\tMovie Title\n",
      "1\t34.0718\t\tAlien\n",
      "2\t34.0718\t\tAliens\n",
      "3\t33.6063\t\tThe Basketball Diaries\n",
      "4\t27.4794\t\tCowboys & Aliens\n",
      "5\t19.8125\t\tAliens vs Predator: Requiem\n",
      "\n",
      "Total results: 48\n",
      "\n",
      "================================================================================\n",
      "STEP 2: DECOMPOSING RELEVANCE SCORES\n",
      "================================================================================\n",
      "\n",
      "Explain for: Alien (Score: 34.0718)\n",
      "--------------------------------------------------------------------------------\n",
      "34.0718 = max of:\n",
      "  34.0718 = sum of:\n",
      "    34.0718 = weight(title:alien in 229) [PerFieldSimilarity], result of:\n",
      "      34.0718 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        10.0000 = boost\n",
      "        5.8273 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          5.0000 = n, number of documents containing term\n",
      "          1866.0000 = N, total number of documents with field\n",
      "        0.5847 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          1.0000 = dl, length of field\n",
      "          2.1935 = avgdl, average length of field\n",
      "  1.5200 = sum of:\n",
      "    1.5200 = weight(overview:alien in 229) [PerFieldSimilarity], result of:\n",
      "      1.5200 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        3.7820 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          42.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.4019 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          48.0000 = dl, length of field (approximate)\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: Aliens (Score: 34.0718)\n",
      "--------------------------------------------------------------------------------\n",
      "34.0718 = max of:\n",
      "  34.0718 = sum of:\n",
      "    34.0718 = weight(title:alien in 454) [PerFieldSimilarity], result of:\n",
      "      34.0718 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        10.0000 = boost\n",
      "        5.8273 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          5.0000 = n, number of documents containing term\n",
      "          1866.0000 = N, total number of documents with field\n",
      "        0.5847 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          1.0000 = dl, length of field\n",
      "          2.1935 = avgdl, average length of field\n",
      "  1.6514 = sum of:\n",
      "    1.6514 = weight(overview:alien in 454) [PerFieldSimilarity], result of:\n",
      "      1.6514 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        3.7820 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          42.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.4366 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          40.0000 = dl, length of field (approximate)\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: The Basketball Diaries (Score: 33.6063)\n",
      "--------------------------------------------------------------------------------\n",
      "33.6063 = max of:\n",
      "  33.6063 = sum of:\n",
      "    33.6063 = weight(title:basketbal in 1346) [PerFieldSimilarity], result of:\n",
      "      33.6063 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        10.0000 = boost\n",
      "        7.1266 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          1.0000 = n, number of documents containing term\n",
      "          1866.0000 = N, total number of documents with field\n",
      "        0.4716 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          2.0000 = dl, length of field\n",
      "          2.1935 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: Cowboys & Aliens (Score: 27.4794)\n",
      "--------------------------------------------------------------------------------\n",
      "27.4794 = max of:\n",
      "  27.4794 = sum of:\n",
      "    27.4794 = weight(title:alien in 465) [PerFieldSimilarity], result of:\n",
      "      27.4794 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        10.0000 = boost\n",
      "        5.8273 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          5.0000 = n, number of documents containing term\n",
      "          1866.0000 = N, total number of documents with field\n",
      "        0.4716 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          2.0000 = dl, length of field\n",
      "          2.1935 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: Aliens vs Predator: Requiem (Score: 19.8125)\n",
      "--------------------------------------------------------------------------------\n",
      "19.8125 = max of:\n",
      "  19.8125 = sum of:\n",
      "    19.8125 = weight(title:alien in 286) [PerFieldSimilarity], result of:\n",
      "      19.8125 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        10.0000 = boost\n",
      "        5.8273 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          5.0000 = n, number of documents containing term\n",
      "          1866.0000 = N, total number of documents with field\n",
      "        0.3400 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          4.0000 = dl, length of field\n",
      "          2.1935 = avgdl, average length of field\n",
      "  2.6605 = sum of:\n",
      "    2.6605 = weight(overview:alien in 286) [PerFieldSimilarity], result of:\n",
      "      2.6605 = score(freq=4.0), computed as boost * idf * tf from:\n",
      "        3.7820 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          42.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.7034 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          4.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          56.0000 = dl, length of field (approximate)\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "STEP 3: FIXING THE RANKING WITH ADJUSTED FIELD WEIGHTS\n",
      "================================================================================\n",
      "Searching with adjusted weights for: 'basketball with cartoon aliens'\n",
      "\n",
      "Search Results:\n",
      "Num\tRelevance Score\t\tMovie Title\n",
      "1\t5.9575\t\tSpace Jam\n",
      "2\t3.2522\t\tThe Flintstones\n",
      "3\t3.1411\t\tWhite Men Can't Jump\n",
      "4\t2.6824\t\tMeet Dave\n",
      "5\t2.6605\t\tAliens vs Predator: Requiem\n",
      "\n",
      "Total results: 48\n",
      "\n",
      "Explaining new ranking with adjusted weights:\n",
      "\n",
      "Explain for: Space Jam (Score: 5.9575)\n",
      "--------------------------------------------------------------------------------\n",
      "5.9575 = max of:\n",
      "  5.9575 = sum of:\n",
      "    3.6606 = weight(overview:basketbal in 1357) [PerFieldSimilarity], result of:\n",
      "      3.6606 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        6.0275 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          4.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.6073 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          14.0000 = dl, length of field\n",
      "          36.3560 = avgdl, average length of field\n",
      "    2.2969 = weight(overview:alien in 1357) [PerFieldSimilarity], result of:\n",
      "      2.2969 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        3.7820 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          42.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.6073 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          14.0000 = dl, length of field\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: The Flintstones (Score: 3.2522)\n",
      "--------------------------------------------------------------------------------\n",
      "3.2522 = max of:\n",
      "  3.2522 = sum of:\n",
      "    3.2522 = weight(overview:cartoon in 601) [PerFieldSimilarity], result of:\n",
      "      3.2522 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        7.1261 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          1.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.4564 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          36.0000 = dl, length of field\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Explain for: White Men Can't Jump (Score: 3.1411)\n",
      "--------------------------------------------------------------------------------\n",
      "3.1411 = max of:\n",
      "  3.1411 = sum of:\n",
      "    3.1411 = weight(overview:basketbal in 1187) [PerFieldSimilarity], result of:\n",
      "      3.1411 = score(freq=1.0), computed as boost * idf * tf from:\n",
      "        6.0275 = idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\n",
      "          4.0000 = n, number of documents containing term\n",
      "          1865.0000 = N, total number of documents with field\n",
      "        0.5211 = tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\n",
      "          1.0000 = freq, occurrences of term within document\n",
      "          1.2000 = k1, term saturation parameter\n",
      "          0.7500 = b, length normalization parameter\n",
      "          25.0000 = dl, length of field\n",
      "          36.3560 = avgdl, average length of field\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "COMPARISON OF RANKINGS\n",
      "================================================================================\n",
      "\n",
      "Original Top 5 (title^10, overview):\n",
      "1. Alien (Score: 34.0718)\n",
      "2. Aliens (Score: 34.0718)\n",
      "3. The Basketball Diaries (Score: 33.6063)\n",
      "4. Cowboys & Aliens (Score: 27.4794)\n",
      "5. Aliens vs Predator: Requiem (Score: 19.8125)\n",
      "\n",
      "Adjusted Top 5 (title^0.1, overview):\n",
      "1. Space Jam (Score: 5.9575)\n",
      "2. The Flintstones (Score: 3.2522)\n",
      "3. White Men Can't Jump (Score: 3.1411)\n",
      "4. Meet Dave (Score: 2.6824)\n",
      "5. Aliens vs Predator: Requiem (Score: 2.6605)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract():\n",
    "    \"\"\"Extract movie data from tmdb.json file\"\"\"\n",
    "    f = open('tmdb.json')\n",
    "    if f:\n",
    "        return json.loads(f.read())        \n",
    "    return {}\n",
    "\n",
    "def setup_client():\n",
    "    \"\"\"Set up and return an OpenSearch client\"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # OpenSearch connection setup\n",
    "    host = 'localhost'\n",
    "    port = 9200\n",
    "    auth = ('admin', os.getenv('OPENSEARCH_INITIAL_ADMIN_PASSWORD'))  # For testing only\n",
    "    \n",
    "    # Create the client with SSL/TLS enabled\n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True,  # enables gzip compression for request bodies\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,  # Set to True in production with proper certs\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    return client\n",
    "\n",
    "def reindex(analysisSettings={}, mappingSettings={}, movieDict={}):\n",
    "    \"\"\"\n",
    "    Reindex the movie data with specified mappings and settings\n",
    "    \"\"\"\n",
    "    client = setup_client()\n",
    "    \n",
    "    # Index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"index\": {\n",
    "                \"analysis\": analysisSettings,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add mappings if provided\n",
    "    if mappingSettings:\n",
    "        # Convert format to match OpenSearch (remove document type layer for OpenSearch ≥ 2.0)\n",
    "        if 'movie' in mappingSettings:\n",
    "            settings['mappings'] = mappingSettings['movie']\n",
    "        else:\n",
    "            settings['mappings'] = mappingSettings\n",
    "    \n",
    "    # Delete index if it exists\n",
    "    try:\n",
    "        client.indices.delete(index='tmdb')\n",
    "        print(\"Deleted existing index\")\n",
    "    except:\n",
    "        pass  # Index might not exist yet\n",
    "    \n",
    "    # Create new index with settings\n",
    "    response = client.indices.create('tmdb', body=settings)\n",
    "    print(\"Created index with mappings:\", response)\n",
    "    \n",
    "    print(\"Building bulk request...\")\n",
    "    bulk_data = []\n",
    "    for id, movie in movieDict.items():\n",
    "        # Add the indexing command\n",
    "        bulk_data.append({\n",
    "            \"index\": {\n",
    "                \"_index\": \"tmdb\",\n",
    "                \"_id\": movie[\"id\"]\n",
    "            }\n",
    "        })\n",
    "        # Add the document to index\n",
    "        bulk_data.append(movie)\n",
    "    \n",
    "    # Only perform bulk operation if there's data to index\n",
    "    if bulk_data:\n",
    "        print(\"Indexing documents...\")\n",
    "        response = client.bulk(body=bulk_data)\n",
    "        \n",
    "        # Check if there were any errors\n",
    "        if response.get('errors', False):\n",
    "            print(\"Errors during bulk indexing:\", response)\n",
    "        else:\n",
    "            print(f\"Successfully indexed {len(bulk_data)//2} documents\")\n",
    "    else:\n",
    "        print(\"No documents to index\")\n",
    "\n",
    "def search(query, display_count=5):\n",
    "    \"\"\"\n",
    "    Search the tmdb index with the given query\n",
    "    \"\"\"\n",
    "    client = setup_client()\n",
    "    \n",
    "    # Perform search using the OpenSearch client\n",
    "    response = client.search(\n",
    "        body=query,\n",
    "        index='tmdb'\n",
    "    )\n",
    "    \n",
    "    # Extract search hits\n",
    "    search_hits = response['hits']\n",
    "    \n",
    "    # Print results in a formatted table\n",
    "    print(\"\\nSearch Results:\")\n",
    "    print(\"Num\\tRelevance Score\\t\\tMovie Title\")\n",
    "    for idx, hit in enumerate(search_hits['hits']):\n",
    "        if idx < display_count:  # Limit displayed results\n",
    "            print(f\"{idx + 1}\\t{hit['_score']:.4f}\\t\\t{hit['_source']['title']}\")\n",
    "    \n",
    "    total_hits = search_hits['total']['value'] if isinstance(search_hits['total'], dict) else search_hits['total']\n",
    "    print(f\"\\nTotal results: {total_hits}\")\n",
    "    \n",
    "    # Return the raw response for further processing\n",
    "    return response\n",
    "\n",
    "def simpler_explain(explanation, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively simplify Lucene's explanation into a more readable format.\n",
    "    Returns a simplified explanation string.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    result = []\n",
    "    \n",
    "    # Extract the main contribution and description\n",
    "    value = explanation.get('value', 0)\n",
    "    description = explanation.get('description', 'Unknown')\n",
    "    \n",
    "    # Add this level's explanation\n",
    "    result.append(f\"{indent}{value:.4f} = {description}\")\n",
    "    \n",
    "    # Process child explanations recursively\n",
    "    if 'details' in explanation and explanation['details']:\n",
    "        for detail in explanation['details']:\n",
    "            result.append(simpler_explain(detail, depth + 1))\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def decompose_relevance_score(query_results, num_docs=3):\n",
    "    \"\"\"\n",
    "    Decompose and explain the relevance scores for the top N results\n",
    "    \"\"\"\n",
    "    if not query_results or 'hits' not in query_results or 'hits' not in query_results['hits']:\n",
    "        print(\"No results to explain\")\n",
    "        return\n",
    "    \n",
    "    hits = query_results['hits']['hits']\n",
    "    \n",
    "    for i, hit in enumerate(hits):\n",
    "        if i >= num_docs:\n",
    "            break\n",
    "            \n",
    "        if '_explanation' in hit:\n",
    "            print(f\"\\nExplain for: {hit['_source']['title']} (Score: {hit['_score']:.4f})\")\n",
    "            print(\"-\" * 80)\n",
    "            print(simpler_explain(hit['_explanation']))\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            print(f\"No explanation available for: {hit['_source']['title']}\")\n",
    "\n",
    "def run_search_workflow():\n",
    "    \"\"\"Run a complete workflow with search, explain, and ranking adjustment\"\"\"\n",
    "    # 1. Define the mapping settings with proper text field types and analyzers\n",
    "    mapping_settings = {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'english'\n",
    "            },\n",
    "            'overview': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'english'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 2. Extract movie data and reindex with proper mappings\n",
    "    movie_dict = extract()\n",
    "    if movie_dict:\n",
    "        reindex(mappingSettings=mapping_settings, movieDict=movie_dict)\n",
    "        \n",
    "        # 3. Set up the search parameters\n",
    "        users_search = 'basketball with cartoon aliens'\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 1: INITIAL SEARCH\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Initial search query\n",
    "        query = {\n",
    "            'query': {\n",
    "                'multi_match': { \n",
    "                    'query': users_search,\n",
    "                    'fields': ['title^10', 'overview'],  # title has 10x the weight\n",
    "                },\n",
    "            },\n",
    "            'size': 100  # Number of results to return\n",
    "        }\n",
    "        \n",
    "        # 4. Execute search and get results\n",
    "        print(f\"Searching for: '{users_search}'\")\n",
    "        response = search(query)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: DECOMPOSING RELEVANCE SCORES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 5. Add explain parameter to get relevance score details\n",
    "        explain_query = query.copy()\n",
    "        explain_query['explain'] = True\n",
    "        \n",
    "        # 6. Execute the query with explain\n",
    "        explain_response = setup_client().search(\n",
    "            body=explain_query,\n",
    "            index='tmdb'\n",
    "        )\n",
    "        \n",
    "        # 7. Decompose and explain the relevance scores\n",
    "        decompose_relevance_score(explain_response, 5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: FIXING THE RANKING WITH ADJUSTED FIELD WEIGHTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 8. Updated query with adjusted field weights\n",
    "        adjusted_query = {\n",
    "            'query': {\n",
    "                'multi_match': { \n",
    "                    'query': users_search,\n",
    "                    'fields': ['title^0.1', 'overview'],  # title now has 0.1x the weight of overview\n",
    "                },\n",
    "            },\n",
    "            'explain': True,\n",
    "            'size': 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Searching with adjusted weights for: '{users_search}'\")\n",
    "        adjusted_response = search(adjusted_query)\n",
    "        \n",
    "        # 9. Explain the new ranking\n",
    "        print(\"\\nExplaining new ranking with adjusted weights:\")\n",
    "        decompose_relevance_score(adjusted_response, 3)\n",
    "        \n",
    "        # 10. Compare results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARISON OF RANKINGS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\nOriginal Top 5 (title^10, overview):\")\n",
    "        for i, hit in enumerate(response['hits']['hits'][:5]):\n",
    "            print(f\"{i+1}. {hit['_source']['title']} (Score: {hit['_score']:.4f})\")\n",
    "            \n",
    "        print(\"\\nAdjusted Top 5 (title^0.1, overview):\")\n",
    "        for i, hit in enumerate(adjusted_response['hits']['hits'][:5]):\n",
    "            print(f\"{i+1}. {hit['_source']['title']} (Score: {hit['_score']:.4f})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No movie data found. Please ensure the tmdb.json file exists.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_search_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796e4d",
   "metadata": {},
   "source": [
    "# 3.4.4\tFixing Space Jam vs Alien Ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
